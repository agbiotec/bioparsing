wget ftp://ftp.uniprot.org/pub/databases/uniprot/uniref/uniref100/uniref100.fasta.gz
wget ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/idmapping/idmapping.dat.gz
wget ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.dat.gz
wget ftp://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_trembl.dat.gz

gunzip *



1). First Hadoop job to get the taxon ids and lineages

--Preparation and data input:
cat uniprot_sprot.dat >> uniprot_trembl.dat (will be used for parsing lineages and taxon ids)
/opt/hadoop/bin/hadoop fs -put uniprot_trembl.dat /data/kkrampis/uniprot/uniprot_trembl_sprot.dat (former is local dir, latter is hadoop dir)

--Run the job:
/opt/hadoop/bin/hadoop jar /opt/hadoop/contrib/streaming/hadoop-0.20.2-streaming.jar -input /data/kkrampis/uniprot/uniprot_trembl_sprot.dat -output /data/kkrampis/uniprot/uniprot_taxon_parsed_NCBI_taxid.dat.2  -file /home/kkrampis/devel/BioDoop/uniprot_hadoop_mapper_parser.rb  -mapper /home/kkrampis/devel/BioDoop/uniprot_hadoop_mapper_parser.rb -numReduceTasks 128
( 4mins, 36sec )

--Export data to local file system:
/opt/hadoop/bin/hadoop fs -get /data/kkrampis/uniprot/uniprot_taxon_parsed_NCBI_taxid.dat.2 /usr/local/scratch/kkrampis (former is hadoop dir, latter is local dir)




2). Prepare the file with Uniref sequences and taxon/lineages for the second Hadoop job

-- Join sequence and header lines to avoid split of a multi-line sequence across the Hadoop file chunks (separation of sequence and header
   is take care by the code).

  awk '/^>/&&NR>1{print "";}{ printf "%s",/^>/ ? $0"\t":$0 }' uniref100.fasta > uniref100.fasta.joined


-- Join cross-reference lines 

  perl idmappings_flatten.pl idmapping.dat >> idmapping.single_line.dat

-- Add taxon/lineages
cd /usr/local/scratch/kkrampis/uniprot_taxon_parsed_NCBI_taxid.dat.2
(notice the 128 part-***** files which are the Hadoop splits -numReduceTasks when processing taxon/lineages in parallel)
cat part-* >> taxonID_mapping.dat

-- Use only the cross-reference id mappings , and taxon mappings that are present in the clusters of Uniref100 (otherwise Hadoop gets confused by the extra keys)

grep \> uniref100.fasta | cut -f 1 -d " " | sed -e 's/>UniRef100_//' | sed -e 's/^/grep /' | sed -e 's/$/ idmapping.single_line.dat >> uniref100.idmappings.dat/' > uniref100.idmappings.sh &

grep \> uniref100.fasta | cut -f 1 -d " " | sed -e 's/>UniRef100_//' | sed -e 's/^/grep /' | sed -e 's/$/ taxonID_mapping.dat >> uniref100.taxonID_mapping.dat/' > uniref100.taxonID_mapping.sh &

./uniref100.idmappings.sh
./uniref100.taxonID_mapping.sh


-- Fix the file that will be loaded on Hadoop
cat uniref100.idmappings.dat >> uniref100.fasta.joined
cat uniref100.taxonID_mapping.dat >> uniref100.fasta.joined


-- How many unique kingdoms we have ?
cut -d '-' -f 4 part-00* | sort -u



 
3). Second Hadoop job

--SE/UniRef_to_Panda/ Copy prepared file into Hadoop filesystem
/opt/hadoop/bin/hadoop fs -put /usr/local/scratch/kkrampis/uniref/uniref100.fasta.joined /data/kkrampis/uniprot2panda/

--Run the job
/opt/hadoop/bin/hadoop jar /opt/hadoop/contrib/streaming/hadoop-0.20.2-streaming.jar -input /data/kkrampis/uniprot2panda/uniref100.fasta.joined -output /data/kkrampis/uniprot2panda/uniref2panda.with.kingdoms -file /home/kkrampis/devel/BioDoop/uniref2panda_map_2.pl  -mapper /home/kkrampis/devel/BioDoop/uniref2panda_map_2.pl -file /home/kkrampis/devel/BioDoop/uniref2panda_reduce_2.pl  -reducer /home/kkrampis/devel/BioDoop/uniref2panda_reduce_2.pl -numReduceTasks 128
(approximately 1 hr)


-- Copy output to local filesystem and concatenate file parts
/opt/hadoop/bin/hadoop fs -get /data/kkrampis/uniprot2panda/uniref2panda.with.kingdoms /usr/local/scratch/kkrampis
cd /usr/local/scratch/kkrampis/uniref2panda.with.kingdoms
cat part-* >> uniref100_PANDA_Hadoop.fasta
cp uniref100_PANDA_Hadoop.fasta /usr/local/projects/PANDA/PANDA_uniref/all/


-- Split by Kingdom
grep -A 1 "---Archaea" uniref100_PANDA_Hadoop.fasta | sed -e 's/---Archaea//' > /usr/local/projects/PANDA/PANDA_uniref/split_by_kingdom/Archaea/uniref100_PANDA_Archaea_Hadoop.fasta
grep -A 1 "---Bacteria" uniref100_PANDA_Hadoop.fasta | sed -e 's/---Bacteria//' > /usr/local/projects/PANDA/PANDA_uniref/split_by_kingdom/Bacteria/uniref100_PANDA_Bacteria_Hadoop.fasta
grep -A 1 "---Eukaryota" uniref100_PANDA_Hadoop.fasta | sed -e 's/---Eukaryota//' > /usr/local/projects/PANDA/PANDA_uniref/split_by_kingdom/Eukaryota/uniref100_PANDA_Eukaryota_Hadoop.fasta
grep -A 1 "---Viruses" uniref100_PANDA_Hadoop.fasta | sed -e 's/---Viruses//' > /usr/local/projects/PANDA/PANDA_uniref/split_by_kingdom/Viruses/uniref100_PANDA_Viruses_Hadoop.fasta
grep -A 1 "---noKingdom" uniref100_PANDA_Hadoop.fasta | sed -e 's/---noKingdom//' > /usr/local/projects/PANDA/PANDA_uniref/split_by_kingdom/noKingdom/uniref100_PANDA_noKingdom_Hadoop.fasta

grep -A 1 "---unclassifiedsequences" uniref100_PANDA_Hadoop.fasta | sed -e 's/---unclassifiedsequences//' >> /usr/local/projects/PANDA/PANDA_uniref/split_by_kingdom/unclassified/uniref100_PANDA_unclassified_Hadoop.fasta
grep -A 1 "---unclassified" uniref100_PANDA_Hadoop.fasta | sed -e 's/---unclassified//' >> /usr/local/projects/PANDA/PANDA_uniref/split_by_kingdom/unclassified/uniref100_PANDA_unclassified_Hadoop.fasta
grep -A 1 "---othersequences" uniref100_PANDA_Hadoop.fasta | sed -e 's/---othersequences//' >> /usr/local/projects/PANDA/PANDA_uniref/split_by_kingdom/unclassified/uniref100_PANDA_unclassified_Hadoop.fasta


-- Format DB
qsub -P 08020 -wd /usr/local/projects/PANDA/PANDA_uniref/split_by_kingdom/Archaea formatdb -i /usr/local/projects/PANDA/PANDA_uniref/split_by_kingdom/Archaea/uniref100_PANDA_Archaea_Hadoop.fasta -o T
